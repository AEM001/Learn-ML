{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7246ce1c",
   "metadata": {},
   "source": [
    "plan\n",
    "- 没有学习过seq2seq\n",
    "- 设计从多个指标通过mlp学习到一个x\n",
    "- 从x预测天气\n",
    "- 通过GRU之类的预测x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9cb195ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天气类型编码（索引=编码，对应标签如下）:\n",
      "[(0, 'drizzle'), (1, 'rain'), (2, 'sun'), (3, 'snow'), (4, 'fog')]\n",
      "   season  precipitation  temp_max  temp_min  wind  weather_code\n",
      "0       1            0.0      12.8       5.0   4.7             0\n",
      "1       1           10.9      10.6       2.8   4.5             1\n",
      "2       1            0.8      11.7       7.2   2.3             1\n",
      "3       1           20.3      12.2       5.6   4.7             1\n",
      "4       1            1.3       8.9       2.8   6.1             1\n",
      "数据已保存到 seattle-weather-processed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "seed=42\n",
    "# 加载数据\n",
    "df = pd.read_csv('/Users/Mac/code/Practice/ML/Kaggle/rnn_weather_pred/seattle-weather.csv')\n",
    "\n",
    "# 将 'date' 列转换为 datetime 类型，方便后续处理\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def month_to_season(month):\n",
    "    \"\"\"将月份转换为季节：1=冬, 2=春, 3=夏, 4=秋\"\"\"\n",
    "    if month in [12, 1, 2]:\n",
    "        return 1  # Winter\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 2  # Spring\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 3  # Summer\n",
    "    else:  # 9, 10, 11\n",
    "        return 4  # Autumn\n",
    "\n",
    "# 生成编码后的季节变量\n",
    "df['season'] = df['date'].dt.month.apply(month_to_season)\n",
    "\n",
    "# 天气编码：保留数值编码，同时保存编码与标签的对应关系\n",
    "df['weather_code'], weather_labels = pd.factorize(df['weather'])\n",
    "\n",
    "# 在内存中删除不需要的原始列：date 和 weather\n",
    "df.drop(columns=['date', 'weather'], inplace=True)\n",
    "\n",
    "# 将 weather_code 调整到最后面，season 放前面\n",
    "front_cols = ['season']\n",
    "other_cols = [c for c in df.columns if c not in front_cols + ['weather_code']]\n",
    "df = df[front_cols + other_cols + ['weather_code']]\n",
    "\n",
    "# 输出检查\n",
    "print(\"天气类型编码（索引=编码，对应标签如下）:\")\n",
    "print(list(enumerate(weather_labels)))\n",
    "print(df.head())\n",
    "\n",
    "# 将处理好的数据保存到新的CSV文件中（不保存索引和表头）\n",
    "df.to_csv('/Users/Mac/code/Practice/ML/Kaggle/rnn_weather_pred/seattle-weather-processed.csv', index=False, header=False)\n",
    "print(\"数据已保存到 seattle-weather-processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9c35241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据形状: x_train torch.Size([1168, 5]), y_train torch.Size([1168, 1])\n",
      "测试数据形状: x_test torch.Size([293, 5]), y_test torch.Size([293, 1])\n"
     ]
    }
   ],
   "source": [
    "data=np.loadtxt('seattle-weather-processed.csv',delimiter=',')\n",
    "split=int(0.8*len(data))\n",
    "x_train,y_train=data[:split,:5],data[:split,5].reshape(-1,1)\n",
    "x_test,y_test=data[split:,:5],data[split:,5].reshape(-1,1)\n",
    "# 转换为PyTorch张量\n",
    "import torch\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "print(f\"训练数据形状: x_train {x_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"测试数据形状: x_test {x_test.shape}, y_test {y_test.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c81d51c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500, Loss: 1.6907, Accuracy: 0.0000\n",
      "Epoch: 2/500, Loss: 1.6610, Accuracy: 0.0068\n",
      "Epoch: 3/500, Loss: 1.6327, Accuracy: 0.0137\n",
      "Epoch: 4/500, Loss: 1.6056, Accuracy: 0.2491\n",
      "Epoch: 5/500, Loss: 1.5797, Accuracy: 0.4334\n",
      "Epoch: 6/500, Loss: 1.5548, Accuracy: 0.4608\n",
      "Epoch: 7/500, Loss: 1.5306, Accuracy: 0.4608\n",
      "Epoch: 8/500, Loss: 1.5070, Accuracy: 0.4676\n",
      "Epoch: 9/500, Loss: 1.4838, Accuracy: 0.4812\n",
      "Epoch: 10/500, Loss: 1.4606, Accuracy: 0.4881\n",
      "Epoch: 11/500, Loss: 1.4374, Accuracy: 0.4915\n",
      "Epoch: 12/500, Loss: 1.4143, Accuracy: 0.4983\n",
      "Epoch: 13/500, Loss: 1.3913, Accuracy: 0.4983\n",
      "Epoch: 14/500, Loss: 1.3684, Accuracy: 0.4983\n",
      "Epoch: 15/500, Loss: 1.3458, Accuracy: 0.5222\n",
      "Epoch: 16/500, Loss: 1.3236, Accuracy: 0.5495\n",
      "Epoch: 17/500, Loss: 1.3018, Accuracy: 0.5631\n",
      "Epoch: 18/500, Loss: 1.2806, Accuracy: 0.5700\n",
      "Epoch: 19/500, Loss: 1.2603, Accuracy: 0.5768\n",
      "Epoch: 20/500, Loss: 1.2407, Accuracy: 0.5836\n",
      "Epoch: 21/500, Loss: 1.2218, Accuracy: 0.5904\n",
      "Epoch: 22/500, Loss: 1.2035, Accuracy: 0.5973\n",
      "Epoch: 23/500, Loss: 1.1859, Accuracy: 0.5973\n",
      "Epoch: 24/500, Loss: 1.1690, Accuracy: 0.6075\n",
      "Epoch: 25/500, Loss: 1.1528, Accuracy: 0.6143\n",
      "Epoch: 26/500, Loss: 1.1372, Accuracy: 0.6212\n",
      "Epoch: 27/500, Loss: 1.1223, Accuracy: 0.6314\n",
      "Epoch: 28/500, Loss: 1.1080, Accuracy: 0.6382\n",
      "Epoch: 29/500, Loss: 1.0943, Accuracy: 0.6485\n",
      "Epoch: 30/500, Loss: 1.0812, Accuracy: 0.6621\n",
      "Epoch: 31/500, Loss: 1.0687, Accuracy: 0.6997\n",
      "Epoch: 32/500, Loss: 1.0568, Accuracy: 0.7133\n",
      "Epoch: 33/500, Loss: 1.0455, Accuracy: 0.7372\n",
      "Epoch: 34/500, Loss: 1.0349, Accuracy: 0.7577\n",
      "Epoch: 35/500, Loss: 1.0249, Accuracy: 0.7509\n",
      "Epoch: 36/500, Loss: 1.0155, Accuracy: 0.7645\n",
      "Epoch: 37/500, Loss: 1.0067, Accuracy: 0.7645\n",
      "Epoch: 38/500, Loss: 0.9982, Accuracy: 0.7713\n",
      "Epoch: 39/500, Loss: 0.9900, Accuracy: 0.7645\n",
      "Epoch: 40/500, Loss: 0.9820, Accuracy: 0.7611\n",
      "Epoch: 41/500, Loss: 0.9741, Accuracy: 0.7611\n",
      "Epoch: 42/500, Loss: 0.9661, Accuracy: 0.7645\n",
      "Epoch: 43/500, Loss: 0.9579, Accuracy: 0.7679\n",
      "Epoch: 44/500, Loss: 0.9497, Accuracy: 0.7645\n",
      "Epoch: 45/500, Loss: 0.9414, Accuracy: 0.7611\n",
      "Epoch: 46/500, Loss: 0.9332, Accuracy: 0.7645\n",
      "Epoch: 47/500, Loss: 0.9251, Accuracy: 0.7645\n",
      "Epoch: 48/500, Loss: 0.9170, Accuracy: 0.7645\n",
      "Epoch: 49/500, Loss: 0.9090, Accuracy: 0.7645\n",
      "Epoch: 50/500, Loss: 0.9012, Accuracy: 0.7645\n",
      "Epoch: 51/500, Loss: 0.8935, Accuracy: 0.7645\n",
      "Epoch: 52/500, Loss: 0.8861, Accuracy: 0.7679\n",
      "Epoch: 53/500, Loss: 0.8787, Accuracy: 0.7679\n",
      "Epoch: 54/500, Loss: 0.8715, Accuracy: 0.7679\n",
      "Epoch: 55/500, Loss: 0.8644, Accuracy: 0.7679\n",
      "Epoch: 56/500, Loss: 0.8574, Accuracy: 0.7679\n",
      "Epoch: 57/500, Loss: 0.8504, Accuracy: 0.7679\n",
      "Epoch: 58/500, Loss: 0.8434, Accuracy: 0.7645\n",
      "Epoch: 59/500, Loss: 0.8366, Accuracy: 0.7645\n",
      "Epoch: 60/500, Loss: 0.8301, Accuracy: 0.7645\n",
      "Epoch: 61/500, Loss: 0.8237, Accuracy: 0.7645\n",
      "Epoch: 62/500, Loss: 0.8175, Accuracy: 0.7611\n",
      "Epoch: 63/500, Loss: 0.8115, Accuracy: 0.7679\n",
      "Epoch: 64/500, Loss: 0.8057, Accuracy: 0.7645\n",
      "Epoch: 65/500, Loss: 0.8001, Accuracy: 0.7645\n",
      "Epoch: 66/500, Loss: 0.7948, Accuracy: 0.7645\n",
      "Epoch: 67/500, Loss: 0.7897, Accuracy: 0.7611\n",
      "Epoch: 68/500, Loss: 0.7847, Accuracy: 0.7611\n",
      "Epoch: 69/500, Loss: 0.7798, Accuracy: 0.7645\n",
      "Epoch: 70/500, Loss: 0.7752, Accuracy: 0.7645\n",
      "Epoch: 71/500, Loss: 0.7706, Accuracy: 0.7611\n",
      "Epoch: 72/500, Loss: 0.7662, Accuracy: 0.7611\n",
      "Epoch: 73/500, Loss: 0.7619, Accuracy: 0.7611\n",
      "Epoch: 74/500, Loss: 0.7577, Accuracy: 0.7611\n",
      "Epoch: 75/500, Loss: 0.7536, Accuracy: 0.7611\n",
      "Epoch: 76/500, Loss: 0.7496, Accuracy: 0.7611\n",
      "Epoch: 77/500, Loss: 0.7455, Accuracy: 0.7645\n",
      "Epoch: 78/500, Loss: 0.7416, Accuracy: 0.7611\n",
      "Epoch: 79/500, Loss: 0.7377, Accuracy: 0.7611\n",
      "Epoch: 80/500, Loss: 0.7339, Accuracy: 0.7611\n",
      "Epoch: 81/500, Loss: 0.7302, Accuracy: 0.7679\n",
      "Epoch: 82/500, Loss: 0.7266, Accuracy: 0.7679\n",
      "Epoch: 83/500, Loss: 0.7232, Accuracy: 0.7713\n",
      "Epoch: 84/500, Loss: 0.7198, Accuracy: 0.7713\n",
      "Epoch: 85/500, Loss: 0.7165, Accuracy: 0.7747\n",
      "Epoch: 86/500, Loss: 0.7133, Accuracy: 0.7747\n",
      "Epoch: 87/500, Loss: 0.7102, Accuracy: 0.7747\n",
      "Epoch: 88/500, Loss: 0.7071, Accuracy: 0.7747\n",
      "Epoch: 89/500, Loss: 0.7041, Accuracy: 0.7782\n",
      "Epoch: 90/500, Loss: 0.7013, Accuracy: 0.7782\n",
      "Epoch: 91/500, Loss: 0.6984, Accuracy: 0.7850\n",
      "Epoch: 92/500, Loss: 0.6955, Accuracy: 0.7850\n",
      "Epoch: 93/500, Loss: 0.6927, Accuracy: 0.7850\n",
      "Epoch: 94/500, Loss: 0.6900, Accuracy: 0.7884\n",
      "Epoch: 95/500, Loss: 0.6873, Accuracy: 0.7884\n",
      "Epoch: 96/500, Loss: 0.6846, Accuracy: 0.7884\n",
      "Epoch: 97/500, Loss: 0.6819, Accuracy: 0.7884\n",
      "Epoch: 98/500, Loss: 0.6792, Accuracy: 0.7884\n",
      "Epoch: 99/500, Loss: 0.6766, Accuracy: 0.7884\n",
      "Epoch: 100/500, Loss: 0.6739, Accuracy: 0.7918\n",
      "Epoch: 101/500, Loss: 0.6713, Accuracy: 0.7918\n",
      "Epoch: 102/500, Loss: 0.6687, Accuracy: 0.7918\n",
      "Epoch: 103/500, Loss: 0.6660, Accuracy: 0.7918\n",
      "Epoch: 104/500, Loss: 0.6634, Accuracy: 0.7918\n",
      "Epoch: 105/500, Loss: 0.6607, Accuracy: 0.7918\n",
      "Epoch: 106/500, Loss: 0.6579, Accuracy: 0.7986\n",
      "Epoch: 107/500, Loss: 0.6552, Accuracy: 0.8055\n",
      "Epoch: 108/500, Loss: 0.6525, Accuracy: 0.8055\n",
      "Epoch: 109/500, Loss: 0.6498, Accuracy: 0.8055\n",
      "Epoch: 110/500, Loss: 0.6471, Accuracy: 0.8055\n",
      "Epoch: 111/500, Loss: 0.6443, Accuracy: 0.8055\n",
      "Epoch: 112/500, Loss: 0.6416, Accuracy: 0.8089\n",
      "Epoch: 113/500, Loss: 0.6390, Accuracy: 0.8123\n",
      "Epoch: 114/500, Loss: 0.6365, Accuracy: 0.8123\n",
      "Epoch: 115/500, Loss: 0.6340, Accuracy: 0.8123\n",
      "Epoch: 116/500, Loss: 0.6315, Accuracy: 0.8123\n",
      "Epoch: 117/500, Loss: 0.6290, Accuracy: 0.8123\n",
      "Epoch: 118/500, Loss: 0.6267, Accuracy: 0.8123\n",
      "Epoch: 119/500, Loss: 0.6244, Accuracy: 0.8123\n",
      "Epoch: 120/500, Loss: 0.6221, Accuracy: 0.8123\n",
      "Epoch: 121/500, Loss: 0.6198, Accuracy: 0.8157\n",
      "Epoch: 122/500, Loss: 0.6176, Accuracy: 0.8123\n",
      "Epoch: 123/500, Loss: 0.6154, Accuracy: 0.8089\n",
      "Epoch: 124/500, Loss: 0.6131, Accuracy: 0.8123\n",
      "Epoch: 125/500, Loss: 0.6109, Accuracy: 0.8123\n",
      "Epoch: 126/500, Loss: 0.6087, Accuracy: 0.8191\n",
      "Epoch: 127/500, Loss: 0.6064, Accuracy: 0.8191\n",
      "Epoch: 128/500, Loss: 0.6043, Accuracy: 0.8225\n",
      "Epoch: 129/500, Loss: 0.6021, Accuracy: 0.8225\n",
      "Epoch: 130/500, Loss: 0.6000, Accuracy: 0.8225\n",
      "Epoch: 131/500, Loss: 0.5979, Accuracy: 0.8225\n",
      "Epoch: 132/500, Loss: 0.5958, Accuracy: 0.8259\n",
      "Epoch: 133/500, Loss: 0.5937, Accuracy: 0.8259\n",
      "Epoch: 134/500, Loss: 0.5916, Accuracy: 0.8259\n",
      "Epoch: 135/500, Loss: 0.5894, Accuracy: 0.8294\n",
      "Epoch: 136/500, Loss: 0.5874, Accuracy: 0.8294\n",
      "Epoch: 137/500, Loss: 0.5853, Accuracy: 0.8259\n",
      "Epoch: 138/500, Loss: 0.5832, Accuracy: 0.8259\n",
      "Epoch: 139/500, Loss: 0.5811, Accuracy: 0.8225\n",
      "Epoch: 140/500, Loss: 0.5790, Accuracy: 0.8225\n",
      "Epoch: 141/500, Loss: 0.5769, Accuracy: 0.8225\n",
      "Epoch: 142/500, Loss: 0.5748, Accuracy: 0.8259\n",
      "Epoch: 143/500, Loss: 0.5726, Accuracy: 0.8259\n",
      "Epoch: 144/500, Loss: 0.5705, Accuracy: 0.8259\n",
      "Epoch: 145/500, Loss: 0.5684, Accuracy: 0.8259\n",
      "Epoch: 146/500, Loss: 0.5664, Accuracy: 0.8259\n",
      "Epoch: 147/500, Loss: 0.5645, Accuracy: 0.8294\n",
      "Epoch: 148/500, Loss: 0.5627, Accuracy: 0.8294\n",
      "Epoch: 149/500, Loss: 0.5609, Accuracy: 0.8328\n",
      "Epoch: 150/500, Loss: 0.5592, Accuracy: 0.8294\n",
      "Epoch: 151/500, Loss: 0.5575, Accuracy: 0.8294\n",
      "Epoch: 152/500, Loss: 0.5558, Accuracy: 0.8328\n",
      "Epoch: 153/500, Loss: 0.5542, Accuracy: 0.8328\n",
      "Epoch: 154/500, Loss: 0.5525, Accuracy: 0.8328\n",
      "Epoch: 155/500, Loss: 0.5509, Accuracy: 0.8328\n",
      "Epoch: 156/500, Loss: 0.5493, Accuracy: 0.8328\n",
      "Epoch: 157/500, Loss: 0.5478, Accuracy: 0.8328\n",
      "Epoch: 158/500, Loss: 0.5462, Accuracy: 0.8328\n",
      "Epoch: 159/500, Loss: 0.5447, Accuracy: 0.8328\n",
      "Epoch: 160/500, Loss: 0.5432, Accuracy: 0.8328\n",
      "Epoch: 161/500, Loss: 0.5417, Accuracy: 0.8328\n",
      "Epoch: 162/500, Loss: 0.5402, Accuracy: 0.8328\n",
      "Epoch: 163/500, Loss: 0.5388, Accuracy: 0.8328\n",
      "Epoch: 164/500, Loss: 0.5374, Accuracy: 0.8328\n",
      "Epoch: 165/500, Loss: 0.5359, Accuracy: 0.8328\n",
      "Epoch: 166/500, Loss: 0.5345, Accuracy: 0.8328\n",
      "Epoch: 167/500, Loss: 0.5331, Accuracy: 0.8294\n",
      "Epoch: 168/500, Loss: 0.5317, Accuracy: 0.8294\n",
      "Epoch: 169/500, Loss: 0.5303, Accuracy: 0.8294\n",
      "Epoch: 170/500, Loss: 0.5289, Accuracy: 0.8294\n",
      "Epoch: 171/500, Loss: 0.5276, Accuracy: 0.8294\n",
      "Epoch: 172/500, Loss: 0.5262, Accuracy: 0.8294\n",
      "Epoch: 173/500, Loss: 0.5249, Accuracy: 0.8294\n",
      "Epoch: 174/500, Loss: 0.5237, Accuracy: 0.8294\n",
      "Epoch: 175/500, Loss: 0.5224, Accuracy: 0.8294\n",
      "Epoch: 176/500, Loss: 0.5212, Accuracy: 0.8328\n",
      "Epoch: 177/500, Loss: 0.5200, Accuracy: 0.8328\n",
      "Epoch: 178/500, Loss: 0.5188, Accuracy: 0.8328\n",
      "Epoch: 179/500, Loss: 0.5176, Accuracy: 0.8328\n",
      "Epoch: 180/500, Loss: 0.5164, Accuracy: 0.8328\n",
      "Epoch: 181/500, Loss: 0.5153, Accuracy: 0.8362\n",
      "Epoch: 182/500, Loss: 0.5142, Accuracy: 0.8328\n",
      "Epoch: 183/500, Loss: 0.5132, Accuracy: 0.8362\n",
      "Epoch: 184/500, Loss: 0.5123, Accuracy: 0.8328\n",
      "Epoch: 185/500, Loss: 0.5112, Accuracy: 0.8362\n",
      "Epoch: 186/500, Loss: 0.5099, Accuracy: 0.8396\n",
      "Epoch: 187/500, Loss: 0.5088, Accuracy: 0.8362\n",
      "Epoch: 188/500, Loss: 0.5081, Accuracy: 0.8430\n",
      "Epoch: 189/500, Loss: 0.5070, Accuracy: 0.8396\n",
      "Epoch: 190/500, Loss: 0.5058, Accuracy: 0.8396\n",
      "Epoch: 191/500, Loss: 0.5049, Accuracy: 0.8430\n",
      "Epoch: 192/500, Loss: 0.5041, Accuracy: 0.8396\n",
      "Epoch: 193/500, Loss: 0.5031, Accuracy: 0.8430\n",
      "Epoch: 194/500, Loss: 0.5021, Accuracy: 0.8430\n",
      "Epoch: 195/500, Loss: 0.5013, Accuracy: 0.8430\n",
      "Epoch: 196/500, Loss: 0.5005, Accuracy: 0.8430\n",
      "Epoch: 197/500, Loss: 0.4995, Accuracy: 0.8430\n",
      "Epoch: 198/500, Loss: 0.4986, Accuracy: 0.8430\n",
      "Epoch: 199/500, Loss: 0.4978, Accuracy: 0.8430\n",
      "Epoch: 200/500, Loss: 0.4970, Accuracy: 0.8430\n",
      "Epoch: 201/500, Loss: 0.4962, Accuracy: 0.8430\n",
      "Epoch: 202/500, Loss: 0.4953, Accuracy: 0.8430\n",
      "Epoch: 203/500, Loss: 0.4945, Accuracy: 0.8430\n",
      "Epoch: 204/500, Loss: 0.4938, Accuracy: 0.8498\n",
      "Epoch: 205/500, Loss: 0.4930, Accuracy: 0.8430\n",
      "Epoch: 206/500, Loss: 0.4922, Accuracy: 0.8498\n",
      "Epoch: 207/500, Loss: 0.4914, Accuracy: 0.8464\n",
      "Epoch: 208/500, Loss: 0.4906, Accuracy: 0.8498\n",
      "Epoch: 209/500, Loss: 0.4898, Accuracy: 0.8498\n",
      "Epoch: 210/500, Loss: 0.4891, Accuracy: 0.8498\n",
      "Epoch: 211/500, Loss: 0.4883, Accuracy: 0.8498\n",
      "Epoch: 212/500, Loss: 0.4876, Accuracy: 0.8498\n",
      "Epoch: 213/500, Loss: 0.4869, Accuracy: 0.8532\n",
      "Epoch: 214/500, Loss: 0.4862, Accuracy: 0.8498\n",
      "Epoch: 215/500, Loss: 0.4856, Accuracy: 0.8567\n",
      "Epoch: 216/500, Loss: 0.4849, Accuracy: 0.8532\n",
      "Epoch: 217/500, Loss: 0.4842, Accuracy: 0.8567\n",
      "Epoch: 218/500, Loss: 0.4835, Accuracy: 0.8532\n",
      "Epoch: 219/500, Loss: 0.4827, Accuracy: 0.8532\n",
      "Epoch: 220/500, Loss: 0.4821, Accuracy: 0.8567\n",
      "Epoch: 221/500, Loss: 0.4814, Accuracy: 0.8532\n",
      "Epoch: 222/500, Loss: 0.4809, Accuracy: 0.8567\n",
      "Epoch: 223/500, Loss: 0.4803, Accuracy: 0.8532\n",
      "Epoch: 224/500, Loss: 0.4797, Accuracy: 0.8601\n",
      "Epoch: 225/500, Loss: 0.4791, Accuracy: 0.8532\n",
      "Epoch: 226/500, Loss: 0.4785, Accuracy: 0.8601\n",
      "Epoch: 227/500, Loss: 0.4779, Accuracy: 0.8567\n",
      "Epoch: 228/500, Loss: 0.4772, Accuracy: 0.8567\n",
      "Epoch: 229/500, Loss: 0.4766, Accuracy: 0.8567\n",
      "Epoch: 230/500, Loss: 0.4760, Accuracy: 0.8567\n",
      "Epoch: 231/500, Loss: 0.4755, Accuracy: 0.8601\n",
      "Epoch: 232/500, Loss: 0.4750, Accuracy: 0.8567\n",
      "Epoch: 233/500, Loss: 0.4745, Accuracy: 0.8601\n",
      "Epoch: 234/500, Loss: 0.4741, Accuracy: 0.8567\n",
      "Epoch: 235/500, Loss: 0.4737, Accuracy: 0.8601\n",
      "Epoch: 236/500, Loss: 0.4733, Accuracy: 0.8567\n",
      "Epoch: 237/500, Loss: 0.4729, Accuracy: 0.8601\n",
      "Epoch: 238/500, Loss: 0.4722, Accuracy: 0.8567\n",
      "Epoch: 239/500, Loss: 0.4714, Accuracy: 0.8601\n",
      "Epoch: 240/500, Loss: 0.4708, Accuracy: 0.8601\n",
      "Epoch: 241/500, Loss: 0.4704, Accuracy: 0.8567\n",
      "Epoch: 242/500, Loss: 0.4701, Accuracy: 0.8601\n",
      "Epoch: 243/500, Loss: 0.4697, Accuracy: 0.8601\n",
      "Epoch: 244/500, Loss: 0.4691, Accuracy: 0.8601\n",
      "Epoch: 245/500, Loss: 0.4684, Accuracy: 0.8601\n",
      "Epoch: 246/500, Loss: 0.4680, Accuracy: 0.8601\n",
      "Epoch: 247/500, Loss: 0.4677, Accuracy: 0.8601\n",
      "Epoch: 248/500, Loss: 0.4673, Accuracy: 0.8601\n",
      "Epoch: 249/500, Loss: 0.4669, Accuracy: 0.8601\n",
      "Epoch: 250/500, Loss: 0.4664, Accuracy: 0.8601\n",
      "Epoch: 251/500, Loss: 0.4658, Accuracy: 0.8601\n",
      "Epoch: 252/500, Loss: 0.4654, Accuracy: 0.8601\n",
      "Epoch: 253/500, Loss: 0.4650, Accuracy: 0.8601\n",
      "Epoch: 254/500, Loss: 0.4647, Accuracy: 0.8669\n",
      "Epoch: 255/500, Loss: 0.4643, Accuracy: 0.8601\n",
      "Epoch: 256/500, Loss: 0.4640, Accuracy: 0.8669\n",
      "Epoch: 257/500, Loss: 0.4635, Accuracy: 0.8601\n",
      "Epoch: 258/500, Loss: 0.4630, Accuracy: 0.8635\n",
      "Epoch: 259/500, Loss: 0.4625, Accuracy: 0.8635\n",
      "Epoch: 260/500, Loss: 0.4621, Accuracy: 0.8635\n",
      "Epoch: 261/500, Loss: 0.4618, Accuracy: 0.8669\n",
      "Epoch: 262/500, Loss: 0.4614, Accuracy: 0.8601\n",
      "Epoch: 263/500, Loss: 0.4611, Accuracy: 0.8669\n",
      "Epoch: 264/500, Loss: 0.4607, Accuracy: 0.8601\n",
      "Epoch: 265/500, Loss: 0.4604, Accuracy: 0.8669\n",
      "Epoch: 266/500, Loss: 0.4600, Accuracy: 0.8635\n",
      "Epoch: 267/500, Loss: 0.4597, Accuracy: 0.8669\n",
      "Epoch: 268/500, Loss: 0.4593, Accuracy: 0.8669\n",
      "Epoch: 269/500, Loss: 0.4590, Accuracy: 0.8669\n",
      "Epoch: 270/500, Loss: 0.4586, Accuracy: 0.8669\n",
      "Epoch: 271/500, Loss: 0.4583, Accuracy: 0.8669\n",
      "Epoch: 272/500, Loss: 0.4579, Accuracy: 0.8669\n",
      "Epoch: 273/500, Loss: 0.4575, Accuracy: 0.8669\n",
      "Epoch: 274/500, Loss: 0.4571, Accuracy: 0.8669\n",
      "Epoch: 275/500, Loss: 0.4568, Accuracy: 0.8669\n",
      "Epoch: 276/500, Loss: 0.4565, Accuracy: 0.8669\n",
      "Epoch: 277/500, Loss: 0.4561, Accuracy: 0.8669\n",
      "Epoch: 278/500, Loss: 0.4558, Accuracy: 0.8669\n",
      "Epoch: 279/500, Loss: 0.4555, Accuracy: 0.8669\n",
      "Epoch: 280/500, Loss: 0.4552, Accuracy: 0.8669\n",
      "Epoch: 281/500, Loss: 0.4549, Accuracy: 0.8669\n",
      "Epoch: 282/500, Loss: 0.4547, Accuracy: 0.8669\n",
      "Epoch: 283/500, Loss: 0.4546, Accuracy: 0.8669\n",
      "Epoch: 284/500, Loss: 0.4545, Accuracy: 0.8669\n",
      "Epoch: 285/500, Loss: 0.4548, Accuracy: 0.8669\n",
      "Epoch: 286/500, Loss: 0.4545, Accuracy: 0.8669\n",
      "Epoch: 287/500, Loss: 0.4541, Accuracy: 0.8669\n",
      "Epoch: 288/500, Loss: 0.4530, Accuracy: 0.8669\n",
      "Epoch: 289/500, Loss: 0.4525, Accuracy: 0.8669\n",
      "Epoch: 290/500, Loss: 0.4526, Accuracy: 0.8669\n",
      "Epoch: 291/500, Loss: 0.4526, Accuracy: 0.8669\n",
      "Epoch: 292/500, Loss: 0.4522, Accuracy: 0.8669\n",
      "Epoch: 293/500, Loss: 0.4515, Accuracy: 0.8669\n",
      "Epoch: 294/500, Loss: 0.4512, Accuracy: 0.8669\n",
      "Epoch: 295/500, Loss: 0.4513, Accuracy: 0.8669\n",
      "Epoch: 296/500, Loss: 0.4509, Accuracy: 0.8669\n",
      "Epoch: 297/500, Loss: 0.4504, Accuracy: 0.8669\n",
      "Epoch: 298/500, Loss: 0.4500, Accuracy: 0.8669\n",
      "Epoch: 299/500, Loss: 0.4499, Accuracy: 0.8669\n",
      "Epoch: 300/500, Loss: 0.4500, Accuracy: 0.8669\n",
      "Epoch: 301/500, Loss: 0.4495, Accuracy: 0.8669\n",
      "Epoch: 302/500, Loss: 0.4490, Accuracy: 0.8669\n",
      "Epoch: 303/500, Loss: 0.4488, Accuracy: 0.8669\n",
      "Epoch: 304/500, Loss: 0.4487, Accuracy: 0.8669\n",
      "Epoch: 305/500, Loss: 0.4486, Accuracy: 0.8669\n",
      "Epoch: 306/500, Loss: 0.4482, Accuracy: 0.8669\n",
      "Epoch: 307/500, Loss: 0.4478, Accuracy: 0.8669\n",
      "Epoch: 308/500, Loss: 0.4475, Accuracy: 0.8669\n",
      "Epoch: 309/500, Loss: 0.4474, Accuracy: 0.8669\n",
      "Epoch: 310/500, Loss: 0.4472, Accuracy: 0.8669\n",
      "Epoch: 311/500, Loss: 0.4469, Accuracy: 0.8669\n",
      "Epoch: 312/500, Loss: 0.4466, Accuracy: 0.8635\n",
      "Epoch: 313/500, Loss: 0.4463, Accuracy: 0.8669\n",
      "Epoch: 314/500, Loss: 0.4461, Accuracy: 0.8669\n",
      "Epoch: 315/500, Loss: 0.4460, Accuracy: 0.8669\n",
      "Epoch: 316/500, Loss: 0.4458, Accuracy: 0.8669\n",
      "Epoch: 317/500, Loss: 0.4455, Accuracy: 0.8669\n",
      "Epoch: 318/500, Loss: 0.4453, Accuracy: 0.8635\n",
      "Epoch: 319/500, Loss: 0.4450, Accuracy: 0.8669\n",
      "Epoch: 320/500, Loss: 0.4447, Accuracy: 0.8669\n",
      "Epoch: 321/500, Loss: 0.4445, Accuracy: 0.8669\n",
      "Epoch: 322/500, Loss: 0.4443, Accuracy: 0.8635\n",
      "Epoch: 323/500, Loss: 0.4441, Accuracy: 0.8635\n",
      "Epoch: 324/500, Loss: 0.4439, Accuracy: 0.8635\n",
      "Epoch: 325/500, Loss: 0.4436, Accuracy: 0.8669\n",
      "Epoch: 326/500, Loss: 0.4434, Accuracy: 0.8635\n",
      "Epoch: 327/500, Loss: 0.4432, Accuracy: 0.8635\n",
      "Epoch: 328/500, Loss: 0.4430, Accuracy: 0.8635\n",
      "Epoch: 329/500, Loss: 0.4428, Accuracy: 0.8635\n",
      "Epoch: 330/500, Loss: 0.4426, Accuracy: 0.8635\n",
      "Epoch: 331/500, Loss: 0.4424, Accuracy: 0.8635\n",
      "Epoch: 332/500, Loss: 0.4422, Accuracy: 0.8635\n",
      "Epoch: 333/500, Loss: 0.4420, Accuracy: 0.8635\n",
      "Epoch: 334/500, Loss: 0.4418, Accuracy: 0.8601\n",
      "Epoch: 335/500, Loss: 0.4416, Accuracy: 0.8601\n",
      "Epoch: 336/500, Loss: 0.4414, Accuracy: 0.8601\n",
      "Epoch: 337/500, Loss: 0.4412, Accuracy: 0.8635\n",
      "Epoch: 338/500, Loss: 0.4410, Accuracy: 0.8601\n",
      "Epoch: 339/500, Loss: 0.4408, Accuracy: 0.8532\n",
      "Epoch: 340/500, Loss: 0.4406, Accuracy: 0.8532\n",
      "Epoch: 341/500, Loss: 0.4404, Accuracy: 0.8532\n",
      "Epoch: 342/500, Loss: 0.4402, Accuracy: 0.8532\n",
      "Epoch: 343/500, Loss: 0.4400, Accuracy: 0.8532\n",
      "Epoch: 344/500, Loss: 0.4399, Accuracy: 0.8567\n",
      "Epoch: 345/500, Loss: 0.4397, Accuracy: 0.8567\n",
      "Epoch: 346/500, Loss: 0.4395, Accuracy: 0.8532\n",
      "Epoch: 347/500, Loss: 0.4393, Accuracy: 0.8532\n",
      "Epoch: 348/500, Loss: 0.4391, Accuracy: 0.8532\n",
      "Epoch: 349/500, Loss: 0.4390, Accuracy: 0.8532\n",
      "Epoch: 350/500, Loss: 0.4388, Accuracy: 0.8601\n",
      "Epoch: 351/500, Loss: 0.4387, Accuracy: 0.8532\n",
      "Epoch: 352/500, Loss: 0.4387, Accuracy: 0.8532\n",
      "Epoch: 353/500, Loss: 0.4388, Accuracy: 0.8532\n",
      "Epoch: 354/500, Loss: 0.4395, Accuracy: 0.8567\n",
      "Epoch: 355/500, Loss: 0.4401, Accuracy: 0.8567\n",
      "Epoch: 356/500, Loss: 0.4417, Accuracy: 0.8567\n",
      "Epoch: 357/500, Loss: 0.4390, Accuracy: 0.8532\n",
      "Epoch: 358/500, Loss: 0.4375, Accuracy: 0.8532\n",
      "Epoch: 359/500, Loss: 0.4379, Accuracy: 0.8567\n",
      "Epoch: 360/500, Loss: 0.4385, Accuracy: 0.8532\n",
      "Epoch: 361/500, Loss: 0.4384, Accuracy: 0.8567\n",
      "Epoch: 362/500, Loss: 0.4370, Accuracy: 0.8567\n",
      "Epoch: 363/500, Loss: 0.4371, Accuracy: 0.8532\n",
      "Epoch: 364/500, Loss: 0.4379, Accuracy: 0.8532\n",
      "Epoch: 365/500, Loss: 0.4369, Accuracy: 0.8532\n",
      "Epoch: 366/500, Loss: 0.4363, Accuracy: 0.8532\n",
      "Epoch: 367/500, Loss: 0.4367, Accuracy: 0.8567\n",
      "Epoch: 368/500, Loss: 0.4367, Accuracy: 0.8532\n",
      "Epoch: 369/500, Loss: 0.4362, Accuracy: 0.8532\n",
      "Epoch: 370/500, Loss: 0.4358, Accuracy: 0.8532\n",
      "Epoch: 371/500, Loss: 0.4361, Accuracy: 0.8532\n",
      "Epoch: 372/500, Loss: 0.4361, Accuracy: 0.8567\n",
      "Epoch: 373/500, Loss: 0.4354, Accuracy: 0.8567\n",
      "Epoch: 374/500, Loss: 0.4354, Accuracy: 0.8532\n",
      "Epoch: 375/500, Loss: 0.4356, Accuracy: 0.8532\n",
      "Epoch: 376/500, Loss: 0.4352, Accuracy: 0.8532\n",
      "Epoch: 377/500, Loss: 0.4348, Accuracy: 0.8532\n",
      "Epoch: 378/500, Loss: 0.4349, Accuracy: 0.8567\n",
      "Epoch: 379/500, Loss: 0.4348, Accuracy: 0.8567\n",
      "Epoch: 380/500, Loss: 0.4346, Accuracy: 0.8567\n",
      "Epoch: 381/500, Loss: 0.4343, Accuracy: 0.8532\n",
      "Epoch: 382/500, Loss: 0.4343, Accuracy: 0.8532\n",
      "Epoch: 383/500, Loss: 0.4342, Accuracy: 0.8532\n",
      "Epoch: 384/500, Loss: 0.4339, Accuracy: 0.8532\n",
      "Epoch: 385/500, Loss: 0.4338, Accuracy: 0.8532\n",
      "Epoch: 386/500, Loss: 0.4337, Accuracy: 0.8532\n",
      "Epoch: 387/500, Loss: 0.4336, Accuracy: 0.8532\n",
      "Epoch: 388/500, Loss: 0.4334, Accuracy: 0.8532\n",
      "Epoch: 389/500, Loss: 0.4333, Accuracy: 0.8567\n",
      "Epoch: 390/500, Loss: 0.4332, Accuracy: 0.8532\n",
      "Epoch: 391/500, Loss: 0.4331, Accuracy: 0.8532\n",
      "Epoch: 392/500, Loss: 0.4329, Accuracy: 0.8532\n",
      "Epoch: 393/500, Loss: 0.4327, Accuracy: 0.8532\n",
      "Epoch: 394/500, Loss: 0.4326, Accuracy: 0.8532\n",
      "Epoch: 395/500, Loss: 0.4326, Accuracy: 0.8532\n",
      "Epoch: 396/500, Loss: 0.4324, Accuracy: 0.8567\n",
      "Epoch: 397/500, Loss: 0.4323, Accuracy: 0.8532\n",
      "Epoch: 398/500, Loss: 0.4321, Accuracy: 0.8532\n",
      "Epoch: 399/500, Loss: 0.4320, Accuracy: 0.8532\n",
      "Epoch: 400/500, Loss: 0.4319, Accuracy: 0.8532\n",
      "Epoch: 401/500, Loss: 0.4318, Accuracy: 0.8532\n",
      "Epoch: 402/500, Loss: 0.4317, Accuracy: 0.8532\n",
      "Epoch: 403/500, Loss: 0.4316, Accuracy: 0.8532\n",
      "Epoch: 404/500, Loss: 0.4314, Accuracy: 0.8532\n",
      "Epoch: 405/500, Loss: 0.4313, Accuracy: 0.8532\n",
      "Epoch: 406/500, Loss: 0.4312, Accuracy: 0.8567\n",
      "Epoch: 407/500, Loss: 0.4311, Accuracy: 0.8532\n",
      "Epoch: 408/500, Loss: 0.4310, Accuracy: 0.8532\n",
      "Epoch: 409/500, Loss: 0.4309, Accuracy: 0.8532\n",
      "Epoch: 410/500, Loss: 0.4308, Accuracy: 0.8567\n",
      "Epoch: 411/500, Loss: 0.4306, Accuracy: 0.8567\n",
      "Epoch: 412/500, Loss: 0.4305, Accuracy: 0.8532\n",
      "Epoch: 413/500, Loss: 0.4304, Accuracy: 0.8532\n",
      "Epoch: 414/500, Loss: 0.4303, Accuracy: 0.8532\n",
      "Epoch: 415/500, Loss: 0.4301, Accuracy: 0.8532\n",
      "Epoch: 416/500, Loss: 0.4300, Accuracy: 0.8567\n",
      "Epoch: 417/500, Loss: 0.4299, Accuracy: 0.8532\n",
      "Epoch: 418/500, Loss: 0.4297, Accuracy: 0.8532\n",
      "Epoch: 419/500, Loss: 0.4296, Accuracy: 0.8532\n",
      "Epoch: 420/500, Loss: 0.4295, Accuracy: 0.8532\n",
      "Epoch: 421/500, Loss: 0.4294, Accuracy: 0.8532\n",
      "Epoch: 422/500, Loss: 0.4293, Accuracy: 0.8532\n",
      "Epoch: 423/500, Loss: 0.4292, Accuracy: 0.8532\n",
      "Epoch: 424/500, Loss: 0.4291, Accuracy: 0.8532\n",
      "Epoch: 425/500, Loss: 0.4290, Accuracy: 0.8532\n",
      "Epoch: 426/500, Loss: 0.4289, Accuracy: 0.8532\n",
      "Epoch: 427/500, Loss: 0.4288, Accuracy: 0.8532\n",
      "Epoch: 428/500, Loss: 0.4287, Accuracy: 0.8532\n",
      "Epoch: 429/500, Loss: 0.4285, Accuracy: 0.8532\n",
      "Epoch: 430/500, Loss: 0.4284, Accuracy: 0.8532\n",
      "Epoch: 431/500, Loss: 0.4283, Accuracy: 0.8532\n",
      "Epoch: 432/500, Loss: 0.4282, Accuracy: 0.8532\n",
      "Epoch: 433/500, Loss: 0.4281, Accuracy: 0.8532\n",
      "Epoch: 434/500, Loss: 0.4280, Accuracy: 0.8532\n",
      "Epoch: 435/500, Loss: 0.4279, Accuracy: 0.8532\n",
      "Epoch: 436/500, Loss: 0.4278, Accuracy: 0.8532\n",
      "Epoch: 437/500, Loss: 0.4277, Accuracy: 0.8532\n",
      "Epoch: 438/500, Loss: 0.4276, Accuracy: 0.8532\n",
      "Epoch: 439/500, Loss: 0.4275, Accuracy: 0.8532\n",
      "Epoch: 440/500, Loss: 0.4274, Accuracy: 0.8532\n",
      "Epoch: 441/500, Loss: 0.4273, Accuracy: 0.8532\n",
      "Epoch: 442/500, Loss: 0.4272, Accuracy: 0.8532\n",
      "Epoch: 443/500, Loss: 0.4271, Accuracy: 0.8532\n",
      "Epoch: 444/500, Loss: 0.4270, Accuracy: 0.8532\n",
      "Epoch: 445/500, Loss: 0.4269, Accuracy: 0.8532\n",
      "Epoch: 446/500, Loss: 0.4269, Accuracy: 0.8532\n",
      "Epoch: 447/500, Loss: 0.4268, Accuracy: 0.8532\n",
      "Epoch: 448/500, Loss: 0.4267, Accuracy: 0.8532\n",
      "Epoch: 449/500, Loss: 0.4267, Accuracy: 0.8532\n",
      "Epoch: 450/500, Loss: 0.4268, Accuracy: 0.8532\n",
      "Epoch: 451/500, Loss: 0.4271, Accuracy: 0.8532\n",
      "Epoch: 452/500, Loss: 0.4273, Accuracy: 0.8532\n",
      "Epoch: 453/500, Loss: 0.4283, Accuracy: 0.8532\n",
      "Epoch: 454/500, Loss: 0.4276, Accuracy: 0.8532\n",
      "Epoch: 455/500, Loss: 0.4274, Accuracy: 0.8532\n",
      "Epoch: 456/500, Loss: 0.4262, Accuracy: 0.8532\n",
      "Epoch: 457/500, Loss: 0.4258, Accuracy: 0.8532\n",
      "Epoch: 458/500, Loss: 0.4263, Accuracy: 0.8532\n",
      "Epoch: 459/500, Loss: 0.4265, Accuracy: 0.8532\n",
      "Epoch: 460/500, Loss: 0.4267, Accuracy: 0.8532\n",
      "Epoch: 461/500, Loss: 0.4258, Accuracy: 0.8532\n",
      "Epoch: 462/500, Loss: 0.4253, Accuracy: 0.8532\n",
      "Epoch: 463/500, Loss: 0.4253, Accuracy: 0.8532\n",
      "Epoch: 464/500, Loss: 0.4255, Accuracy: 0.8532\n",
      "Epoch: 465/500, Loss: 0.4256, Accuracy: 0.8532\n",
      "Epoch: 466/500, Loss: 0.4252, Accuracy: 0.8532\n",
      "Epoch: 467/500, Loss: 0.4249, Accuracy: 0.8532\n",
      "Epoch: 468/500, Loss: 0.4248, Accuracy: 0.8532\n",
      "Epoch: 469/500, Loss: 0.4249, Accuracy: 0.8532\n",
      "Epoch: 470/500, Loss: 0.4249, Accuracy: 0.8532\n",
      "Epoch: 471/500, Loss: 0.4247, Accuracy: 0.8532\n",
      "Epoch: 472/500, Loss: 0.4244, Accuracy: 0.8532\n",
      "Epoch: 473/500, Loss: 0.4243, Accuracy: 0.8532\n",
      "Epoch: 474/500, Loss: 0.4243, Accuracy: 0.8532\n",
      "Epoch: 475/500, Loss: 0.4243, Accuracy: 0.8532\n",
      "Epoch: 476/500, Loss: 0.4242, Accuracy: 0.8532\n",
      "Epoch: 477/500, Loss: 0.4240, Accuracy: 0.8532\n",
      "Epoch: 478/500, Loss: 0.4239, Accuracy: 0.8532\n",
      "Epoch: 479/500, Loss: 0.4238, Accuracy: 0.8532\n",
      "Epoch: 480/500, Loss: 0.4237, Accuracy: 0.8532\n",
      "Epoch: 481/500, Loss: 0.4236, Accuracy: 0.8532\n",
      "Epoch: 482/500, Loss: 0.4236, Accuracy: 0.8532\n",
      "Epoch: 483/500, Loss: 0.4235, Accuracy: 0.8532\n",
      "Epoch: 484/500, Loss: 0.4233, Accuracy: 0.8532\n",
      "Epoch: 485/500, Loss: 0.4232, Accuracy: 0.8532\n",
      "Epoch: 486/500, Loss: 0.4231, Accuracy: 0.8532\n",
      "Epoch: 487/500, Loss: 0.4231, Accuracy: 0.8532\n",
      "Epoch: 488/500, Loss: 0.4230, Accuracy: 0.8532\n",
      "Epoch: 489/500, Loss: 0.4230, Accuracy: 0.8532\n",
      "Epoch: 490/500, Loss: 0.4228, Accuracy: 0.8532\n",
      "Epoch: 491/500, Loss: 0.4227, Accuracy: 0.8532\n",
      "Epoch: 492/500, Loss: 0.4226, Accuracy: 0.8532\n",
      "Epoch: 493/500, Loss: 0.4225, Accuracy: 0.8532\n",
      "Epoch: 494/500, Loss: 0.4224, Accuracy: 0.8532\n",
      "Epoch: 495/500, Loss: 0.4224, Accuracy: 0.8532\n",
      "Epoch: 496/500, Loss: 0.4223, Accuracy: 0.8532\n",
      "Epoch: 497/500, Loss: 0.4223, Accuracy: 0.8532\n",
      "Epoch: 498/500, Loss: 0.4222, Accuracy: 0.8532\n",
      "Epoch: 499/500, Loss: 0.4221, Accuracy: 0.8532\n",
      "Epoch: 500/500, Loss: 0.4220, Accuracy: 0.8532\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "input_size=5\n",
    "output_size=1\n",
    "learning_rate=1e-3\n",
    "hidden_1 = 16\n",
    "hidden_2 = 16\n",
    "max_epoch=500\n",
    "num_classes=5\n",
    "# mlp = nn.Sequential(\n",
    "#     nn.Linear(input_size, hidden_1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(hidden_1, hidden_2),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(hidden_2, output_size)\n",
    "# )\n",
    "# mlp_optim = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
    "# for x,y in zip(x_train,y_train):\n",
    "\n",
    "#定义为Encoder和decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_1=32, x_dim=16):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_1, x_dim),\n",
    "            nn.ReLU(),  # 也可不加\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # x_t\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, x_dim, num_classes, hidden_dim_clf=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(x_dim, hidden_dim_clf),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_clf, num_classes)\n",
    "        )\n",
    "    def forward(self, x_t):\n",
    "        return self.net(x_t)\n",
    "\n",
    "encoder = Encoder(input_size, hidden_1=32, x_dim=16)\n",
    "clf = Classifier(16, num_classes)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(clf.parameters()), lr=learning_rate)\n",
    "acc = []\n",
    "criterion = nn.CrossEntropyLoss()  # Assuming classification task\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "for epoch in range(max_epoch):\n",
    "    # 训练阶段\n",
    "    encoder.train()\n",
    "    clf.train()\n",
    "    \n",
    "    x_t = encoder(x_train)              # 编码得到潜在表示\n",
    "    logits = clf(x_t)\n",
    "    # 确保 y_train 是 1D 张量\n",
    "    loss = criterion(logits, y_train.squeeze()) \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 评估阶段\n",
    "    with torch.inference_mode():\n",
    "        encoder.eval()\n",
    "        clf.eval()\n",
    "        y_pred = clf(encoder(x_test))\n",
    "        # 计算准确率并添加到acc列表\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        accuracy = (predicted == y_test.squeeze()).sum().item() / len(y_test)\n",
    "        acc.append(accuracy)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}/{max_epoch}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6d2c9778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 生成全部数据的 latent x ---\n",
    "encoder.eval()\n",
    "with torch.inference_mode():\n",
    "    # 用训练好的编码器分别处理训练和测试数据\n",
    "    x_train_encoded = encoder(x_train)  # 编码训练数据\n",
    "    x_test_encoded = encoder(x_test)    # 编码测试数据\n",
    "\n",
    "# def prepare_data(data,seq_len):\n",
    "#     num_data=len(data)//(seq_len+1)*(seq_len+1)\n",
    "#     data=data[:num_data]\n",
    "#     x=data[:-1].reshape(_1,seq_len,1)\n",
    "#     y=data[1:].reshape(_1,seq_len,1)\n",
    "# 应该是转换为序列对\n",
    "\n",
    "\n",
    "def prepare_data(data,seq_len):\n",
    "    num_data=len(data)-seq_len\n",
    "    x_seq=[]\n",
    "    pre_seq=[]\n",
    "    for i in range(num_data):\n",
    "        x_seq.append(data[i:i+seq_len])\n",
    "        pre_seq.append(data[i+1:i+seq_len+1])\n",
    "    return torch.stack(x_seq),torch.stack(pre_seq)\n",
    "\n",
    "\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "\n",
    "        self.gru=nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc=nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self,x,h=None):\n",
    "        if h is None:\n",
    "            h = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "\n",
    "        out,h=self.gru(x,h)\n",
    "        pred=self.fc(out)\n",
    "        return pred,h\n",
    "\n",
    "    def predict(self,inital,pred_len):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            current_seq=inital.clone()\n",
    "            h=None\n",
    "            outputs=[]\n",
    "\n",
    "            _,h=self.gru(current_seq,h)\n",
    "            x_input=current_seq[:,-1:,:]\n",
    "\n",
    "            for _ in range(pred_len):\n",
    "                y_pred,h=self(x_input,h)\n",
    "                outputs.append(y_pred)\n",
    "\n",
    "                x_input=y_pred\n",
    "\n",
    "            return torch.cat(outputs,dim=1)\n",
    "\n",
    "def train_gru(model,x_train,y_train,epochs=100,lr=1e-3,batch_size=32):\n",
    "        optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "        criterion=nn.MSELoss()\n",
    "\n",
    "        if not isinstance(x_train,torch.Tensor):\n",
    "            x_train=torch.FloatTensor(x_train)\n",
    "            y_train=torch.FloatTensor(y_train)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            indices=torch.randperm(len(x_train))\n",
    "            total_loss=0\n",
    "            \n",
    "            for i in range(0,len(x_train),batch_size):\n",
    "                batch_indices=indices[i:i+batch_size]\n",
    "                x_batch=x_train[batch_indices]\n",
    "                y_batch=y_train[batch_indices]\n",
    "\n",
    "                y_pred,_=model(x_batch)\n",
    "                loss=criterion(y_pred,y_batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss+=loss.item()\n",
    "\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(x_train):.6f}')\n",
    "    \n",
    "        return model\n",
    "\n",
    "def evaluate_gru(model,x_test,y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if not isinstance(x_test,torch.Tensor):\n",
    "            x_test=torch.FloatTensor(x_test)\n",
    "            y_test=torch.FloatTensor(y_test)\n",
    "\n",
    "        y_pred,_=model(x_test)\n",
    "        mse=nn.MSELoss()(y_pred,y_test).item()\n",
    "\n",
    "            # 计算预测准确率（如果是分类问题）\n",
    "        if y_test.dim() > 2 and y_test.size(-1) > 1:  # 多分类情况\n",
    "            pred_classes = torch.argmax(y_pred, dim=-1)\n",
    "            true_classes = torch.argmax(y_test, dim=-1)\n",
    "            accuracy = (pred_classes == true_classes).float().mean().item()\n",
    "            print(f'Test MSE: {mse:.6f}, Accuracy: {accuracy:.4f}')\n",
    "        else:  # 回归问题\n",
    "            print(f'Test MSE: {mse:.6f}')\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2060b870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/400, Loss: 0.334533\n",
      "Epoch 40/400, Loss: 0.304648\n",
      "Epoch 60/400, Loss: 0.285047\n",
      "Epoch 80/400, Loss: 0.245367\n",
      "Epoch 100/400, Loss: 0.216428\n",
      "Epoch 120/400, Loss: 0.200978\n",
      "Epoch 140/400, Loss: 0.180620\n",
      "Epoch 160/400, Loss: 0.166681\n",
      "Epoch 180/400, Loss: 0.156438\n",
      "Epoch 200/400, Loss: 0.150212\n",
      "Epoch 220/400, Loss: 0.141974\n",
      "Epoch 240/400, Loss: 0.140440\n",
      "Epoch 260/400, Loss: 0.134800\n",
      "Epoch 280/400, Loss: 0.130354\n",
      "Epoch 300/400, Loss: 0.128276\n",
      "Epoch 320/400, Loss: 0.124708\n",
      "Epoch 340/400, Loss: 0.122075\n",
      "Epoch 360/400, Loss: 0.119145\n",
      "Epoch 380/400, Loss: 0.116920\n",
      "Epoch 400/400, Loss: 0.117597\n",
      "Test MSE: 16.996431, Accuracy: 0.7897\n",
      "torch.Size([1, 7, 16])\n"
     ]
    }
   ],
   "source": [
    "seq_len=7\n",
    "\n",
    "x_train,y_train=prepare_data(x_train_encoded,seq_len)\n",
    "x_test,y_test=prepare_data(x_test_encoded,seq_len)\n",
    "\n",
    "input_size=16\n",
    "hidden_size=32\n",
    "output_size=16\n",
    "\n",
    "model=GRU(input_size,hidden_size,output_size)\n",
    "\n",
    "model=train_gru(model,x_train,y_train,epochs=400)\n",
    "y_pred=evaluate_gru(model,x_test,y_test)\n",
    "\n",
    "initial_seq=torch.FloatTensor(x_test[:1])\n",
    "future_preds=model.predict(initial_seq,pred_len=7)\n",
    "\n",
    "print(future_preds.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
