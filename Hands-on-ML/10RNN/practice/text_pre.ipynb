{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e33b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果没装 datasets，请先在一个单元里安装：\n",
    "# !pip install -q datasets\n",
    "\n",
    "import torch\n",
    "from practice.data_prep import prepare_shakespeare_loaders\n",
    "\n",
    "# 超参可按需调整\n",
    "seq_len = 100\n",
    "batch_size = 2\n",
    "\n",
    "loaders, tokenizer, vocab_size = prepare_shakespeare_loaders(\n",
    "    seq_len=seq_len,\n",
    "    batch_size=batch_size,\n",
    "    val_ratio=0.1,     # 可调，按连续token流切分\n",
    "    seed=42,\n",
    "    num_workers=0,     # Jupyter 下通常设 0\n",
    "    shuffle_train=True\n",
    ")\n",
    "\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "\n",
    "# 取一个 batch 查看形状与内容\n",
    "xb, yb = next(iter(loaders[\"train\"]))\n",
    "print(\"xb shape:\", xb.shape)  # (batch_size, seq_len)\n",
    "print(\"yb shape:\", yb.shape)  # (batch_size, seq_len)\n",
    "print(\"xb dtype:\", xb.dtype)  # torch.long\n",
    "\n",
    "# 反解码看一下文本是否合理\n",
    "print(\"sample x[0]:\")\n",
    "print(tokenizer.decode(xb[0].tolist()))\n",
    "print(\"sample y[0]:\")\n",
    "print(tokenizer.decode(yb[0].tolist()))\n",
    "\n",
    "# 设备放置（训练时使用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "# 后续你可以用 vocab_size 初始化 Embedding/GRU 等\n",
    "# 例如：\n",
    "# import torch.nn as nn\n",
    "# emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=256).to(device)\n",
    "# gru = nn.GRU(input_size=256, hidden_size=512, batch_first=True).to(device)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
